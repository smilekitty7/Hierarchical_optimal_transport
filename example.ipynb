{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'bbcsport-emd_tr_te_split.mat'\n",
    "'twitter-emd_tr_te_split.mat'\n",
    "'r8-emd_tr_te3.mat'\n",
    "'amazon-emd_tr_te_split.mat'\n",
    "'classic-emd_tr_te_split.mat'\n",
    "'ohsumed-emd_tr_te_ix.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "import ot\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import scipy.io as sio\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "# load data from the WMD paper\n",
    "# each data file contains the words, the embedding vector for each word, the bow vector for each word,\n",
    "\n",
    "\n",
    "# Reduce vocabulary size by stemming and removing stop words.\n",
    "def reduce_vocab(bow_data, vocab, embed_vocab, embed_aggregate='mean'):\n",
    "    \"\"\"Reduce vocabulary size by stemming and removing stop words.\n",
    "    \"\"\"\n",
    "    vocab = np.array(vocab)\n",
    "    short = np.array([len(w) > 2 for w in vocab])\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop = np.array([w not in stop_words for w in vocab])\n",
    "    reduced_vocab = vocab[np.logical_and(short, stop)]\n",
    "    reduced_bow_data = bow_data[:, np.logical_and(short, stop)]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_dict = {}\n",
    "    stemmed_idx_mapping = {}\n",
    "    stemmed_vocab = []\n",
    "    for i, w in enumerate(reduced_vocab):\n",
    "        stem_w = stemmer.stem(w)\n",
    "        if stem_w in stemmed_vocab:\n",
    "            stemmed_dict[stem_w].append(w)\n",
    "            stemmed_idx_mapping[stemmed_vocab.index(stem_w)].append(i)\n",
    "        else:\n",
    "            stemmed_dict[stem_w] = [w]\n",
    "            stemmed_vocab.append(stem_w)\n",
    "            stemmed_idx_mapping[stemmed_vocab.index(stem_w)] = [i]\n",
    "\n",
    "    stemmed_bow_data = np.zeros((bow_data.shape[0], len(stemmed_vocab)),\n",
    "                                dtype=np.int)\n",
    "    for i in range(len(stemmed_vocab)):\n",
    "        stemmed_bow_data[:, i] = reduced_bow_data[:, stemmed_idx_mapping[i]].sum(axis=1).flatten()\n",
    "\n",
    "    word_counts = stemmed_bow_data.sum(axis=0)\n",
    "    stemmed_reduced_vocab = np.array(stemmed_vocab)[word_counts > 2].tolist()\n",
    "    stemmed_reduced_bow_data = stemmed_bow_data[:, word_counts > 2]\n",
    "\n",
    "    stemmed_reduced_embed_vocab = {}\n",
    "    for w in stemmed_reduced_vocab:\n",
    "        old_w_embed = [embed_vocab[w_old] for w_old in stemmed_dict[w]]\n",
    "        if embed_aggregate == 'mean':\n",
    "            new_w_embed = np.mean(old_w_embed, axis=0)\n",
    "        elif embed_aggregate == 'first':\n",
    "            new_w_embed = old_w_embed[0]\n",
    "        else:\n",
    "            print('Unknown embedding aggregation')\n",
    "            break\n",
    "        stemmed_reduced_embed_vocab[w] = new_w_embed\n",
    "\n",
    "    return (stemmed_reduced_vocab,\n",
    "            stemmed_reduced_embed_vocab,\n",
    "            stemmed_reduced_bow_data)\n",
    "\n",
    "\n",
    "def change_embeddings(vocab, bow_data, embed_path):\n",
    "    \"\"\"Change embedding data if vocabulary has been reduced.\"\"\"\n",
    "    all_embed_vocab = {}\n",
    "    with open(embed_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            word = line.split(' ')[0]\n",
    "            embedding = [float(x) for x in line.split(' ')[1:]]\n",
    "            all_embed_vocab[word] = embedding\n",
    "\n",
    "    data_embed_vocab = {}\n",
    "    new_vocab_idx = []\n",
    "    new_vocab = []\n",
    "    for i, w in enumerate(vocab):\n",
    "        if w in all_embed_vocab:\n",
    "            data_embed_vocab[w] = all_embed_vocab[w]\n",
    "            new_vocab_idx.append(i)\n",
    "            new_vocab.append(w)\n",
    "    bow_data = bow_data[:, new_vocab_idx]\n",
    "    return new_vocab, data_embed_vocab, bow_data\n",
    "  \n",
    "# loader\n",
    "def loader(data_path,\n",
    "           embeddings_path,\n",
    "           p=1,\n",
    "           K_lda=70,\n",
    "           glove_embeddings=True,\n",
    "           stemming=True):\n",
    "\n",
    "\n",
    "    data_all = sio.loadmat('./data/bbcsport-emd_tr_te_split.mat', squeeze_me=True, chars_as_strings=True)  # dict\n",
    "\n",
    "    if 'Y' in data_all:\n",
    "        y_all = data_all['Y'].astype(np.int)\n",
    "    else:\n",
    "        y_all = np.concatenate((data_all['yte'].astype(np.int), data_all['ytr'].astype(np.int)), axis=1)\n",
    "\n",
    "    if 'X' in data_all:\n",
    "        embed_all = data_all['X']\n",
    "    else:\n",
    "        embed_all = np.concatenate((data_all['xte'], data_all['xtr']), axis=1)\n",
    "\n",
    "    if 'BOW_X' in data_all:\n",
    "        BOW_all = data_all['BOW_X']\n",
    "    else:\n",
    "        BOW_all = np.concatenate((data_all['BOW_xte'], data_all['BOW_xtr']), axis=1)\n",
    "\n",
    "    if 'words' in data_all:\n",
    "        words_all = data_all['words']\n",
    "    else:\n",
    "        words_all = np.concatenate((data_all['words_tr'], data_all['words_te']), axis=1)\n",
    "\n",
    "    vocab = []\n",
    "    vocab_embed = {}\n",
    "\n",
    "    l = len(words_all)\n",
    "    for i in range(l):\n",
    "        word_i = words_all[i]\n",
    "        embed_i = embed_all[i]\n",
    "        bow_i = BOW_all[i]\n",
    "        w = len(word_i)\n",
    "        for j in range(w):\n",
    "            if type(word_i[j]) == str:\n",
    "                if word_i[j] not in vocab:\n",
    "                    vocab.append(word_i[j])\n",
    "                    vocab_embed[word_i[j]] = embed_i[:, j]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    vocab_BOW = np.zeros((l, len(vocab)), dtype=np.int)\n",
    "\n",
    "    l = len(words_all)\n",
    "    for i in range(l):\n",
    "        word_i = words_all[i]\n",
    "        bow_i = BOW_all[i]\n",
    "\n",
    "        w = len(word_i)\n",
    "        words_idx = []\n",
    "        for j in range(w):\n",
    "            if type(word_i[j]) == str:\n",
    "                words_idx.append(vocab.index(word_i[j]))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        vocab_BOW[i, words_idx] = bow_i.astype(np.int)\n",
    "\n",
    "    ####################################################\n",
    "    # Use GLOVE word embeddings\n",
    "    if glove_embeddings:\n",
    "        vocab, vocab_embed, vocab_BOW = change_embeddings(\n",
    "            vocab, vocab_BOW, embeddings_path)\n",
    "    # Reduce vocabulary by removing short words, stop words, and stemming\n",
    "    if stemming:\n",
    "        vocab, vocab_embed, vocab_BOW = reduce_vocab(\n",
    "            vocab_BOW, vocab, vocab_embed, embed_aggregate='mean')\n",
    "\n",
    "\n",
    "    ####################################################\n",
    "    return vocab_BOW, y_all-1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download datasets used by Kusner et al from\n",
    "# https://www.dropbox.com/sh/nf532hddgdt68ix/AABGLUiPRyXv6UL2YAcHmAFqa?dl=0\n",
    "# and put them into\n",
    "data_path = './data/'\n",
    "\n",
    "# Download GloVe 6B tokens, 300d word embeddings from\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# and put them into\n",
    "embeddings_path = './glove/glove.6B.300d.txt'\n",
    "data = loader(data_path, embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "def wmd(u,v): ### u and v take two probability distributions; weight perhaps take NBOW\n",
    "\n",
    "   return wasserstein_distance(u,v)\n",
    "def wmdT20(u,v): \n",
    "    idx_u = np.argsort(u)\n",
    "    idx_v = np.argsort(v)\n",
    "    u_t20 = np.zeros(len(idx_u[:20]))\n",
    "    v_t20 = np.zeros(len(idx_v[:20]))\n",
    "    k = 0\n",
    "    n = 0\n",
    "    \n",
    "    for i in idx_u[:20]:\n",
    "        u_t20[k] = u[i]\n",
    "\n",
    "        k+=1\n",
    "    for ii in idx_v[:20]:\n",
    "        v_t20[n] = v[ii]\n",
    "\n",
    "        n+=1\n",
    "\n",
    "    return wasserstein_distance(u_t20,v_t20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train, bow_test, y_train, y_test = train_test_split(vocab_BOW, y)\n",
    "nbow_train, nbow_test = normalize(bow_train, 'l1'), normalize(bow_test, 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build classifer\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "def knnclassify(test,x_train,y_train,D,k,method):\n",
    "    distances=[]\n",
    "    for x in range(len(x_train)):\n",
    "        if method == HOFTT:\n",
    "            dist = method(test, x_train[x],D)\n",
    "        else:\n",
    "            dist = method(test, x_train[x])\n",
    "        \n",
    "        distances.append(dist)\n",
    "  \n",
    "\n",
    "    neighbor_inx=np.argsort(distances)[:k]\n",
    "    neighbors = []\n",
    "    for x in range(len(neighbor_inx)):\n",
    "        \n",
    "        neighbors.append(y_train[neighbor_inx[x]])  \n",
    "    \n",
    "#    classvote={}\n",
    "#    nclass = np.unique(neighbors)\n",
    "#    for i in range(len(nclass)):\n",
    "#        temp=neighbors.count(nclass[i])\n",
    "#        classvote.append(temp)\n",
    "#        \n",
    "#    predict=np.argmax(classvote)\n",
    "#    \n",
    "    most_common = Counter(neighbors).most_common(1)\n",
    "    return most_common[0][0]\n",
    "\n",
    "def myknn(k,x_train,y_train,x_test,y_test,D,method):\n",
    "    y_pred=[]\n",
    "    tempcount = 0\n",
    "    for i in range(len(x_test)):\n",
    "        ypred_t=knnclassify(x_test[i],x_train,y_train,D,k,method)\n",
    "        y_pred.append(ypred_t)\n",
    "        tempcount = tempcount+1\n",
    "        print(\"count: \", tempcount)\n",
    "    wait = input(\"PRESS ENTER TO CONTINUE.\")\n",
    "    count=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_pred[i]==y_test[i]:\n",
    "            count=count+1\n",
    "            \n",
    "            \n",
    "    test_error=1-count/len(y_test)\n",
    "    \n",
    "    return test_error\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create own knn with our designed distance\n",
    "#k is neighbors number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test error\n",
    "test_error = myknn(7,nbow_train, y_train,nbow_test, y_test,0, wmdT20)\n",
    "print(test_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
